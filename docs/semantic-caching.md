# Semantic Caching

Quasar implements a sophisticated semantic caching system inspired by remote build cache technologies like Bazel and SBT. Rather than simple byte-level caching, Quasar caches the results of deterministic transformations over immutable, content-addressed blobs and documents.

## Conceptual Foundation

### Function-Based Caching Model
Semantic caching treats every operation as a pure function:
```
f(inputs, parameters) â†’ (result, sideEffects)
```

Where:
- **inputs**: Content-addressed references to immutable blobs or documents
- **parameters**: Hermetic configuration (no ambient state)
- **result**: Primary transformation output
- **sideEffects**: Explicitly declared secondary outputs

### Content-Addressable Everything
Every piece of data is addressed by its cryptographic hash:
- **Input Stability**: Inputs are immutable and uniquely identified
- **Result Uniqueness**: Same inputs always produce same outputs
- **Deduplication**: Identical content stored only once
- **Integrity**: Cryptographic verification of all data

## Cache Key Generation

### Merkle Tree Construction
Cache keys are generated by constructing Merkle trees from operation inputs:

#### Input Canonicalization
1. **Schema Normalization**: Convert inputs to canonical schema representation
2. **Reference Resolution**: Resolve all URNs to content addresses
3. **Parameter Sorting**: Sort parameters deterministically
4. **Metadata Flattening**: Flatten nested metadata structures

#### Hash Computation
```scala
def computeCacheKey(
  operationId: String,
  inputs: Chunk[ContentRef], // BlobKey or DocumentKey
  parameters: DynamicValue,
  schemaVersion: String
): CacheKey = {
  val inputHash = Hash.merkleTree(inputs.map(_.contentHash))
  val paramHash = Hash.canonical(parameters)
  Hash.combine(operationId, inputHash, paramHash, schemaVersion)
}
```

### Hermetic Requirements
All cached operations must be hermetic:

#### Forbidden Dependencies
- **Current timestamp**: No `System.currentTimeMillis()` in cache keys
- **Machine identity**: No hostname, IP address, or hardware fingerprints
- **Environment variables**: No dependency on ambient environment
- **File system state**: No dependency on file existence or permissions

#### Required Practices
- **Explicit inputs**: All dependencies declared as content references
- **Deterministic parameters**: Configuration must be fully specified
- **Reproducible outputs**: Same inputs must produce identical outputs
- **Version pinning**: All external dependencies must be versioned

## Output Declaration System

### Explicit Output Management
Following SBT's `Def.declareOutput` pattern:

#### Declaration Types
```scala
sealed trait OutputDeclaration
case class PrimaryOutput(contentType: MediaType, estimatedSize: Long)
case class DerivedArtifact(name: String, contentType: MediaType)
case class IntermediateResult(cacheKey: CacheKey, ttl: Duration)
case class MetadataUpdate(namespace: String, schema: SchemaRef)
```

#### Benefits of Explicit Declaration
- **Granularity Control**: Choose optimal caching granularity per operation
- **Network Optimization**: Transfer only declared outputs
- **Dependency Tracking**: Clear understanding of operation effects
- **Cache Invalidation**: Precise invalidation of dependent computations

### Content-Addressable Storage Integration
All outputs are stored in Graviton's content-addressable blob storage:

#### Storage Strategy
- **Blob-Level Storage**: Large outputs split into multiple blobs
- **Deduplication**: Identical outputs shared across operations
- **Compression**: Automatic compression based on content type
- **Replication**: Configurable redundancy for critical results

#### Reference Management
- **BlobKey References**: Outputs referenced by content hash
- **FileLayout Support**: Multi-blob outputs with structured layouts
- **Metadata Attachment**: Rich metadata associated with outputs
- **Provenance Tracking**: Link outputs back to generating operations

## Cache Backend Architecture

### Unified Cache Interface
Simplified interface supporting both local and remote caching:

```scala
trait SemanticCacheStore {
  def put[A](
    cacheKey: CacheKey,
    result: A,
    outputs: Chunk[OutputDeclaration]
  ): UIO[CacheEntry[A]]
  
  def get[A](cacheKey: CacheKey): UIO[Option[CacheEntry[A]]]
  
  def putBlobs(blobs: Chunk[(BlobKey, Bytes)]): UIO[Unit]
  
  def getBlobs(keys: Chunk[BlobKey]): UIO[Chunk[Bytes]]
  
  def materialize(
    entry: CacheEntry[_], 
    targetPath: Path
  ): UIO[Chunk[Path]]
}
```

### Cache Hierarchy
Multi-level caching for optimal performance:

#### L1: Memory Cache
- **Location**: In-process memory
- **Size**: 100MB - 1GB per process
- **Latency**: Sub-microsecond access
- **Content**: Small results, metadata, frequently accessed items

#### L2: Local Disk Cache
- **Location**: Local SSD storage
- **Size**: 10GB - 100GB per node
- **Latency**: Single-digit milliseconds
- **Content**: Medium-sized results, working set data

#### L3: Distributed Cache
- **Location**: Cluster-wide shared cache
- **Size**: 100GB - 1TB across cluster
- **Latency**: Low single-digit milliseconds
- **Content**: Shared results, collaborative work

#### L4: Remote Storage
- **Location**: Cloud object storage
- **Size**: Unlimited
- **Latency**: 10-100 milliseconds
- **Content**: Long-term cache, CI/CD artifacts

## Practical Applications

### Blob-Level Operations

#### Content Processing
```scala
case class BlobCompressionOp(
  sourceBlob: BlobKey,
  algorithm: CompressionAlgorithm,
  compressionLevel: Int
)

// Cache key includes blob hash + compression parameters
val cacheKey = computeCacheKey(
  "blob-compression-v1",
  Chunk(sourceBlob),
  BlobCompressionOp(...),
  "2024.1"
)
```

#### Format Conversion
```scala
case class ImageConversionOp(
  sourceBlob: BlobKey,
  targetFormat: ImageFormat,
  quality: Option[Int],
  resizeParams: Option[ResizeParams]
)

// Cached result can be reused across different documents
val convertedBlob = cache.get(cacheKey).getOrElse {
  val result = performConversion(...)
  cache.put(cacheKey, result, outputs)
  result
}
```

### Document-Level Operations

#### Document Processing Pipeline
```scala
case class PDFTextExtractionOp(
  sourceDocument: DocumentKey, // References multiple blobs
  extractionMode: ExtractionMode,
  languageHint: Option[Language],
  preserveFormatting: Boolean
)

// Cache key includes document semantic identity + parameters
val cacheKey = computeCacheKey(
  "pdf-text-extraction-v3",
  Chunk(sourceDocument),
  PDFTextExtractionOp(...),
  "2024.1"
)
```

#### Metadata Enhancement
```scala
case class SchemaValidationOp(
  document: DocumentKey,
  schema: SchemaRef,
  validationLevel: ValidationLevel,
  errorHandling: ErrorHandling
)

// Validation results cached by document + schema combination
val validationResult = semanticCache.cached(
  SchemaValidationOp(...),
  document => validateAgainstSchema(document, schema)
)
```

### Cross-Layer Operations

#### Blob-to-Document Transformation
```scala
case class DocumentCreationOp(
  sourceBlobs: Chunk[BlobKey],
  documentSchema: SchemaRef,
  metadata: DocumentMetadata,
  layoutStrategy: LayoutStrategy
)

// Creates semantic document from raw blobs
val document = semanticCache.cached(
  DocumentCreationOp(...),
  blobs => createDocumentFromBlobs(blobs, schema, metadata)
)
```

#### Document-to-Blob Materialization
```scala
case class DocumentMaterializationOp(
  sourceDocument: DocumentKey,
  outputFormat: OutputFormat,
  compressionLevel: Option[Int]
)

// Materializes document as optimized blob(s)
val materializedBlobs = semanticCache.cached(
  DocumentMaterializationOp(...),
  document => materializeDocument(document, format)
)
```

## Performance Optimization

### Cache Granularity Strategy

#### Blob-Level Granularity
- **Fine-Grained**: Cache individual blob transformations
- **High Reuse**: Same blob content reused across many documents
- **Network Chattiness**: Many small cache operations
- **Optimal For**: Content processing, format conversion, compression

#### Document-Level Granularity
- **Coarse-Grained**: Cache complete document operations
- **Semantic Optimization**: Leverage document-level semantics
- **Fewer Network Calls**: Batch related operations
- **Optimal For**: Complex transformations, multi-step pipelines

#### Adaptive Strategy
```scala
def chooseGranularity(operation: Operation): CacheGranularity = {
  operation match {
    case _: BlobTransformation => BlobLevel
    case _: DocumentAnalysis => DocumentLevel
    case _: ComplexPipeline => AdaptiveLevel(operation.complexity)
  }
}
```

### Latency Optimization

#### Predictive Caching
- **Access Pattern Analysis**: Learn from historical access patterns
- **Dependency Prefetching**: Pre-cache likely dependencies
- **Batch Operations**: Group related cache operations
- **Background Warming**: Warm cache during idle periods

#### Network Optimization
- **Compression**: Compress cache data during transfer
- **Delta Sync**: Transfer only changed portions
- **Parallel Transfers**: Fetch multiple items concurrently
- **Connection Pooling**: Reuse network connections

## Quality Assurance

### Hermeticity Validation

#### Automated Testing
- **Reproducibility Tests**: Verify operations produce identical results
- **Environment Isolation**: Test across different machines and environments
- **Parameter Sensitivity**: Test with various parameter combinations
- **Dependency Analysis**: Verify all dependencies are explicitly declared

#### Static Analysis
- **Code Review**: Review operations for hermetic violations
- **Dependency Scanning**: Scan for forbidden dependencies
- **Configuration Validation**: Validate operation parameters
- **Schema Compliance**: Ensure inputs/outputs match declared schemas

### Cache Coherence

#### Consistency Mechanisms
- **Hash Verification**: Verify cached results match expected hashes
- **Timestamp Tracking**: Track cache entry creation and access times
- **Version Compatibility**: Handle schema version evolution
- **Corruption Detection**: Detect and recover from cache corruption

#### Monitoring and Alerting
- **Hit Rate Monitoring**: Track cache effectiveness across operation types
- **Performance Metrics**: Monitor cache latency and throughput
- **Error Tracking**: Track cache misses, errors, and inconsistencies
- **Capacity Planning**: Monitor cache usage and growth trends

## Advanced Features

### Incremental Computation

#### Change Propagation
- **Minimal Invalidation**: Invalidate only affected cache entries
- **Dependency Tracking**: Track which operations depend on which inputs
- **Delta Processing**: Process only changed portions of large datasets
- **Checkpoint Recovery**: Resume interrupted computations

#### Smart Invalidation
- **Semantic Analysis**: Understand which changes affect which operations
- **Batch Invalidation**: Group related invalidations for efficiency
- **Lazy Invalidation**: Defer invalidation until cache access
- **Predictive Invalidation**: Anticipate future invalidations

### Distributed Coordination

#### Multi-Node Caching
- **Consistent Hashing**: Distribute cache entries across nodes
- **Replication**: Replicate critical cache entries
- **Load Balancing**: Balance cache load across available nodes
- **Fault Tolerance**: Handle node failures gracefully

#### Cross-Cluster Sharing
- **Federation**: Share cache across multiple clusters
- **Synchronization**: Synchronize cache state across geographic regions
- **Conflict Resolution**: Handle concurrent updates to cache entries
- **Bandwidth Management**: Optimize inter-cluster cache transfers

## Implementation Considerations

### Storage Efficiency

#### Compression Strategy
- **Content-Aware Compression**: Choose compression based on content type
- **Dictionary Compression**: Share compression dictionaries across similar content
- **Streaming Compression**: Compress large outputs during generation
- **Decompression Caching**: Cache decompressed data for faster access

#### Garbage Collection
- **LRU Eviction**: Remove least recently used cache entries
- **TTL-Based Cleanup**: Remove expired cache entries
- **Reference Counting**: Clean up unreferenced outputs
- **Compaction**: Compact cache storage periodically

### Security Considerations

#### Access Control
- **Permission-Based Caching**: Respect access permissions in cache
- **Isolation**: Isolate cache entries by security context
- **Audit Logging**: Log all cache access for security auditing
- **Encryption**: Encrypt sensitive cached data

#### Data Protection
- **Sanitization**: Remove sensitive data from cache keys
- **Anonymization**: Anonymize cached data when possible
- **Retention Policies**: Implement data retention policies
- **Secure Deletion**: Securely delete expired cache entries

## Key Insights from SBT Remote Cache Design

### Core Architectural Patterns

#### 1. Function = (Inputs) â†’ (Result + SideEffects)
SBT's key insight: treat every cacheable operation as a pure function with explicit input/output contracts. This maps perfectly to Quasar's view system where transformations are deterministic functions over immutable content.

#### 2. HashedVirtualFileRef â‰ˆ BlobKey
SBT's `HashedVirtualFileRef` combines file path with content hash - exactly what Quasar's `BlobKey` provides. This enables:
- Content-addressable references that survive file system changes
- Deduplication across different logical contexts
- Integrity verification without accessing the actual content

#### 3. Def.declareOutput = Explicit Side Effects
SBT requires explicit declaration of file outputs to solve the "side effect problem" in caching. Quasar adopts this pattern for:
- **View Operations**: Must declare all outputs (blobs, metadata, derived artifacts)
- **Cache Granularity**: Control what gets cached and transferred
- **Dependency Tracking**: Clear understanding of operation effects

#### 4. Merkle Trees for Cache Keys
SBT constructs Merkle trees from the operation's abstract syntax tree. Quasar uses ZIO Schema to:
- Canonicalize input structures for consistent hashing
- Handle schema evolution in cache keys
- Provide type-safe serialization of complex parameters

#### 5. Hermeticity is Critical
SBT's emphasis on hermetic builds (no machine-specific state) directly applies to Quasar:
- **No ambient state**: No current time, machine ID, environment variables
- **Explicit dependencies**: All inputs must be declared as content references
- **Reproducible results**: Same inputs must always produce identical outputs

### Performance Lessons

#### Cache Granularity Tradeoffs
SBT's JAR vs .class files dilemma applies to Quasar:
- **Fine-grained**: Cache individual blob transformations (high reuse, network chattiness)
- **Coarse-grained**: Cache complete document results (less reuse, fewer network calls)
- **Solution**: Let operations declare optimal granularity based on use case

#### Network Optimization
SBT's focus on reducing "file download chattiness" translates to:
- **Batch operations**: Group related cache operations
- **Composite outputs**: Sometimes better to cache aggregated results
- **Predictive fetching**: Pre-fetch likely dependencies

#### Backend Simplicity
SBT's cache backend needs only 5 methods - a lesson in interface design:
- Keep the cache abstraction simple and focused
- Separate concerns: storage vs. coordination vs. policy
- Enable multiple backend implementations with minimal interface

### Practical Applications

#### Blob Processing Pipelines
- **Image â†’ Thumbnail**: Cache by (image blob hash + thumbnail parameters)
- **Document â†’ PDF**: Cache by (document hash + PDF generation parameters)
- **Video â†’ Frames**: Cache by (video blob hash + frame extraction parameters)

#### Document Operations
- **Schema validation**: Cache by (document hash + schema version)
- **Content classification**: Cache by (content hash + model version)
- **Relationship extraction**: Cache by (document set hash + extraction rules)

#### Cross-Layer Operations
- **Blob â†’ Document**: Cache document creation from raw blobs
- **Document â†’ Optimized Blobs**: Cache materialization for different use cases
- **Multi-Document Analysis**: Cache results of operations across document sets

This design enables Quasar to achieve the same dramatic speedups that SBT users report: operations that once took minutes can complete in seconds through intelligent semantic caching at both the blob and document levels. 